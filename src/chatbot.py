{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde2dc90-d7d7-4dd8-b73b-47669cf5aefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file: chatbot.py\n",
    "import json\n",
    "import random\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "import numpy as np\n",
    "import pickle\n",
    "from flask import Flask, request, jsonify, render_template_string\n",
    "\n",
    "# Initialize the lemmatizer and load intents\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "with open('intents.json') as file:\n",
    "    intents = json.load(file)\n",
    "\n",
    "# Preprocessing\n",
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_words = ['?', '!', '.', ',']\n",
    "\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        word_list = nltk.word_tokenize(pattern)\n",
    "        words.extend(word_list)\n",
    "        documents.append((word_list, intent['tag']))\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])\n",
    "\n",
    "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n",
    "words = sorted(set(words))\n",
    "classes = sorted(set(classes))\n",
    "\n",
    "# Training Data Preparation\n",
    "training = []\n",
    "output_empty = [0] * len(classes)\n",
    "for doc in documents:\n",
    "    bag = []\n",
    "    word_patterns = doc[0]\n",
    "    word_patterns = [lemmatizer.lemmatize(word.lower()) for word in word_patterns]\n",
    "    for word in words:\n",
    "        bag.append(1) if word in word_patterns else bag.append(0)\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "    training.append([bag, output_row])\n",
    "\n",
    "random.shuffle(training)\n",
    "training = np.array(training, dtype=object)\n",
    "train_x = np.array(list(training[:, 0]), dtype=object)\n",
    "train_y = np.array(list(training[:, 1]), dtype=object)\n",
    "\n",
    "# Model Definition\n",
    "model = Sequential([\n",
    "    Input(shape=(len(train_x[0]),)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(classes), activation='softmax')\n",
    "])\n",
    "\n",
    "sgd = SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "model.fit(np.array(train_x.tolist()), np.array(train_y.tolist()), epochs=200, batch_size=5, verbose=1)\n",
    "model.save('chatbot_model.keras')\n",
    "\n",
    "# Save necessary data\n",
    "pickle.dump(words, open('words.pkl', 'wb'))\n",
    "pickle.dump(classes, open('classes.pkl', 'wb'))\n",
    "\n",
    "# Flask App for Chatbot\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Home route to avoid 404 errors\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return render_template_string(\"<h1>Chatbot API</h1><p>Use POST /chat with a JSON payload to interact with the chatbot.</p>\")\n",
    "\n",
    "@app.route('/chat', methods=['POST'])\n",
    "def chat():\n",
    "    message = request.json['message']\n",
    "    ints = predict_class(message)\n",
    "    response = get_response(ints, intents)\n",
    "    return jsonify({\"response\": response})\n",
    "\n",
    "def predict_class(sentence):\n",
    "    sentence_words = nltk.word_tokenize(sentence)\n",
    "    sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]\n",
    "    bag = [0] * len(words)\n",
    "    for s in sentence_words:\n",
    "        for i, word in enumerate(words):\n",
    "            if word == s:\n",
    "                bag[i] = 1\n",
    "    res = model.predict(np.array([bag]))[0]\n",
    "    ERROR_THRESHOLD = 0.25\n",
    "    results = [[i, r] for i, r in enumerate(res) if r > ERROR_THRESHOLD]\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return_list = []\n",
    "    for r in results:\n",
    "        return_list.append({\"intent\": classes[r[0]], \"probability\": str(r[1])})\n",
    "    return return_list\n",
    "\n",
    "def get_response(ints, intents_json):\n",
    "    tag = ints[0]['intent']\n",
    "    list_of_intents = intents_json['intents']\n",
    "    for i in list_of_intents:\n",
    "        if i['tag'] == tag:\n",
    "            return random.choice(i['responses'])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fbca3e-cd38-409b-b753-0a4e28064cef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
